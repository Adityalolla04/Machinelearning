{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCxsvmkbnPQr"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark\n",
        "!wget -q http://apache.mirrors.tds.net/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kk6BjMgDoIK1"
      },
      "outputs": [],
      "source": [
        "%env PYSPARK_HADOOP_VERSION=3\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-tiKSmorD-B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtLhLvztoTeI"
      },
      "outputs": [],
      "source": [
        "!PYSPARK_RELEASE_MIRROR=http://mirror.apache-kr.org PYSPARK_HADOOP_VERSION=3 pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncoCs1xQo_Ym"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Colab\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4epu3IwrZ1X"
      },
      "outputs": [],
      "source": [
        "!pip install plotly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9_jI3VrXBRN"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "\n",
        "\n",
        "def read_graph_from_file(filename):\n",
        "    graph = {}\n",
        "    with open(filename, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split(',')\n",
        "            source = int(parts[0].strip())\n",
        "            target = int(parts[1].strip())\n",
        "            weight = int(parts[2].strip())\n",
        "\n",
        "    return graph\n",
        "\n",
        "def dijkstra_length_and_path(graph, source, target):\n",
        "    distances = {node: float('inf') for node in graph}\n",
        "    distances[source] = 0\n",
        "    priority_queue = [(0, source)]\n",
        "\n",
        "    paths = {source: []}\n",
        "\n",
        "    while priority_queue:\n",
        "        current_distance, current_node = heapq.heappop(priority_queue)\n",
        "\n",
        "        if current_node == target:\n",
        "            break\n",
        "\n",
        "        for neighbor, weight in graph[current_node].items():\n",
        "            distance = current_distance + weight\n",
        "\n",
        "            if distance < distances[neighbor]:\n",
        "                distances[neighbor] = distance\n",
        "                heapq.heappush(priority_queue, (distance, neighbor))\n",
        "                paths[neighbor] = paths[current_node] + [current_node]\n",
        "\n",
        "    if distances[target] == float('inf'):\n",
        "        return [], float('inf')\n",
        "\n",
        "    path = paths[target] + [target]\n",
        "\n",
        "    return path, distances[target]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUYRg2zwli7S"
      },
      "outputs": [],
      "source": [
        "graph = read_graph_from_file(\"/content/drive/MyDrive/Colab Notebooks/DIC Assignment-3/question1.txt\")\n",
        "print(graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv3w7nEIWEpj"
      },
      "outputs": [],
      "source": [
        "\n",
        "f = open(\"output_1.txt\", \"w\")\n",
        "for i in range(1,len(graph)):\n",
        "  path, distance = dijkstra_length_and_path(graph, 0, i)\n",
        "  f.write(f\"Shortest path from node 0 to node {i}: {path}\\n\")\n",
        "  f.write(f\"Total distance: {distance}\\n\")\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msNej74aCHTt"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "def main(filename):\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"PageRank Example\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    \n",
        "    edges = spark.read.text(filename) \\\n",
        "        .rdd.map(lambda row: row.value.split(': ')) \\\n",
        "        .map(lambda x: (int(x[0]), list(map(int, x[1].strip('[]').split(', '))))) \\\n",
        "        .flatMapValues(lambda x: x) \\\n",
        "        .map(lambda x: (x[0], x[1]))\n",
        "\n",
        "    \n",
        "    edges_df = edges.toDF([\"src\", \"dst\"])\n",
        "\n",
        "\n",
        "    from graphframes import GraphFrame\n",
        "    vertices = edges_df.selectExpr(\"src as id\").distinct()\n",
        "    g = GraphFrame(vertices, edges_df)\n",
        "\n",
        "   \n",
        "    results = g.pageRank(resetProbability=0.15, maxIter=10)\n",
        "\n",
        "    \n",
        "    max_pagerank = results.vertices.orderBy('pagerank', ascending=False).first()\n",
        "    min_pagerank = results.vertices.orderBy('pagerank', ascending=True).first()\n",
        "\n",
        "    print(f\"Highest PageRank: Node {max_pagerank['id']} with rank {max_pagerank['pagerank']:.4f}\")\n",
        "    print(f\"Lowest PageRank: Node {min_pagerank['id']} with rank {min_pagerank['pagerank']:.4f}\")\n",
        "\n",
        "    spark.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main('/content/drive/MyDrive/Colab Notebooks/DIC Assignment-3/question2.txt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGDDjiTDtGh0"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.mirrors.tds.net/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWcpXcv1vUpN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
